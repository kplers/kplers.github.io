---
title: "PyTorch-2: "
excerpt: ""

categories:
  - python
tags:
  - [PyTorch]

permalink: /python/pytorch2

toc: true
toc_sticky: true

date: 2024-11-30
last_modified_at: 2024-11-30
---

**차례**

- [파이토치 nn 모듈](#파이토치-nn-모듈)
  - [배치 최적화](#배치-최적화)
- [드디어 신경망!](#드디어-신경망)
- [결론](#결론)

___

본 에피소드에서는 '신경망'이 무엇인지는 알고 있다고 가정하고 진행한다. 신경망의 개념과 관련된 내용은 다른 카테고리의 에피소드에서 나중에 다룰 예정이다.

## 파이토치 nn 모듈

파이토치에는 torch.nn이라는 신경망 전용 서브모듈이 있다. 이 모듈에는 모든 신경망 아키텍처를 만들 수 있는 빌딩 블럭, '모듈(module)'이 들어 있다. (다른 프레임워크 및 이론에서는 계층(layer)이라 함)

파이토치 모듈은 nn.Module 베이스 클래스에서 파생된 파이썬 클래스로, 하나 이상의 Parameter 객체를 인자로 받는다. 이는 텐서 타입이며 훈련을 통해 값이 최적화된다.

모듈은 다시 하나 이상의 서브 모듈을 속성으로 가지며 파라미터 추적이 가능하다.

우리가 5장(에피소드 1)에서 사용했던 선형 모델(y=wx+b)도 있다! `nn.Linear` 서브클래스다. 5장에서 만든 코드를 `nn`을 사용하는 형태로 바꿔보자!

`nn.Module`의 모든 서브클래스에는 `__call__` 메소드가 정의되어 있어, 굳이 `forward`를 사용하지 않고 함수인 것처럼 실행할 수 있다. 예시를 보자.


```python
%matplotlib inline
import numpy as np
import torch
import torch.optim as optim

torch.set_printoptions(edgeitems=2, linewidth=75)
t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]
t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]
t_c = torch.tensor(t_c).unsqueeze(1) # <1>
t_u = torch.tensor(t_u).unsqueeze(1) # <1>

n_samples = t_u.shape[0]
n_val = int(0.2 * n_samples)

shuffled_indices = torch.randperm(n_samples)

train_indices = shuffled_indices[:-n_val]
val_indices = shuffled_indices[-n_val:]

t_u_train = t_u[train_indices]
t_c_train = t_c[train_indices]

t_u_val = t_u[val_indices]
t_c_val = t_c[val_indices]

t_un_train = 0.1 * t_u_train
t_un_val = 0.1 * t_u_val
```


```python
import torch.nn as nn

linear_model = nn.Linear(1, 1)
linear_model(t_un_val)
```




    tensor([[3.8989],
            [7.2737]], grad_fn=<AddmmBackward0>)



`forward`를 사용하는 것보다 그냥 `__call__`을 이용하는 편이 낫다. `__call__`에는 `forward` 외에도 여러 중요한 작업이 있기 때문이다.

nn.Linear 생성자는 세 개의 인자를 받는다. 입력 피처의 수, 출력 피처의 수, 그리고 선형 모델이 편향값을 포함하는지 여부(기본값 True)다.

우리의 경우 입력 피처와 출력 피처의 수 모두 1이다. (온도 하나만을 입력받고 하나만을 출력하니까.)

나중에 보겠지만 피처의 수는 모델의 용량과 연관이 있다.

이제 하나의 입력 피처와 하나의 출력 피처를 가진 `nn.Linear` 인스턴스가 있다. 하나의 가중치와 편향값이 요구된다.


```python
linear_model.weight
```




    Parameter containing:
    tensor([[0.9782]], requires_grad=True)




```python
linear_model.bias
```




    Parameter containing:
    tensor([0.5828], requires_grad=True)



이제 입력값으로 모듈을 호출해보자.


```python
x = torch.ones(1)    # x = tensor([1.])
linear_model(x)
```




    tensor([1.5610], grad_fn=<ViewBackward0>)



파이토치 모듈들은 한 번에 여러 개의 샘플을 다루도록 설계되었다. 따라서 모듈 입력의 0번째 차원은 배치에 들어있는 샘플의 수가 된다. 즉, 엄밀히 말하면 위의 입력의 차원은 정확하지 않다. 정확하게는 

`tensor([[1.]])`

를 입력으로 넣어야 할 것이다. 하지만 똑똑한 파이토치는 알아서 잘 한다.

이제 10개의 샘플에 대해 nn.Linear를 실행한다고 가정해보자. B가 배치의 크기, Nin이 입력 피처의 크기라면 입력 텐서의 차원은 B * Nin으로 만들면 된다.

그러면 배치 안에 있는 각각의 샘플마다 모델에 넣어 출력값을 도출하여 텐서로 출력해 준다.

아래의 코드 실행 결과를 보면 알 수 있다.


```python
x = torch.ones(10, 1)
linear_model(x)
```




    tensor([[1.5610],
            [1.5610],
            [1.5610],
            [1.5610],
            [1.5610],
            [1.5610],
            [1.5610],
            [1.5610],
            [1.5610],
            [1.5610]], grad_fn=<AddmmBackward0>)



### 배치 최적화

앞서 이미 t_c와 t_u를 unsqueeze하여 다음과 같이 여분의 1번째 차원을 만들어 놓았다.


```python
t_c, t_u
```




    (tensor([[ 0.5000],
             [14.0000],
             [15.0000],
             [28.0000],
             [11.0000],
             [ 8.0000],
             [ 3.0000],
             [-4.0000],
             [ 6.0000],
             [13.0000],
             [21.0000]]),
     tensor([[35.7000],
             [55.9000],
             [58.2000],
             [81.9000],
             [56.3000],
             [48.9000],
             [33.9000],
             [21.8000],
             [48.4000],
             [60.4000],
             [68.4000]]))



이러면 모델에 입력으로 넣을 수 있다! 이제 nn.Linear(1, 1) 모델을 사용하고 옵티마이저에 파라미터를 전달하자.


```python
linear_model = nn.Linear(1, 1)
optimizer = optim.SGD(linear_model.parameters(), lr=1e-2)
```

보다시피 파라미터는 그냥 모델에 .parameters()를 활용하면 된다. 더 이상 직접 만들 필요가 없다.


```python
linear_model.parameters()
```




    <generator object Module.parameters at 0x000001BE1BB497E0>




```python
list(linear_model.parameters())
```




    [Parameter containing:
     tensor([[-0.6258]], requires_grad=True),
     Parameter containing:
     tensor([-0.2388], requires_grad=True)]




```python
def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_c_train, t_u_val, t_c_val):
    for epoch in range(1, n_epochs+1):
        t_p_train = model(t_u_train)
        loss_train = loss_fn(t_p_train, t_c_train)

        t_p_val = model(t_u_val)
        loss_val = loss_fn(t_p_val, t_c_val)

        optimizer.zero_grad()  # 파라미터의 grad를 0으로 초기화한다.
        loss_train.backward()  # 역전사를 통해 파라미터의 grad를 구한다.
        optimizer.step()  # grad에 따라 파라미터를 조정한다.

        if epoch == 1 or epoch % 1000 == 0:
            print(f"Epoch {epoch}, Training loss {loss_train.item():.4f},"
                  f"Validation loss {loss_val.item():.4f}")
```


```python
training_loop(
    n_epochs=3000,
    optimizer=optimizer,
    model=linear_model,
    loss_fn=nn.MSELoss(),  # 이제 직접 만든 손실 함수는 사용하지 않아도 된다!
    t_u_train=t_un_train,
    t_c_train=t_c_train,
    t_u_val=t_un_val,
    t_c_val=t_c_val
)

print()
print(linear_model.weight)
print(linear_model.bias)
```

    Epoch 1, Training loss 279.9994,Validation loss 339.9886
    Epoch 1000, Training loss 3.6943,Validation loss 4.2364
    Epoch 2000, Training loss 2.6682,Validation loss 4.8495
    Epoch 3000, Training loss 2.6293,Validation loss 5.1635
    
    Parameter containing:
    tensor([[5.3971]], requires_grad=True)
    Parameter containing:
    tensor([-17.8573], requires_grad=True)
    

잘 됐다!

## 드디어 신경망!

드디어 신경망을 사용할 시간이다! 선형 모델에는 한계가 많기 때문에 이것을 신경망으로 대체하는 것이다. 이론적으로 신경망은 거의 모든 함수를 표현할 수 있기 때문에 선형 모델과는 비교도 되지 않을 정도로 강력하다.

물론 우리의 온도 문제는 실제로 선형 모델을 따르기 때문에 결과가 좋아지지 않는다. 하지만 괜찮다! 연습이니까.

다른 건 그대로 두고 model을 바꿔야 한다. 알다시피 신경망은 여러 층으로 구성되어 있다. 우리가 구현할 제일 단순한 신경망은 다음과 같다.

입력 => 선형 모듈 => 활성화 함수(tanh) => 선형 모듈 => 출력

nn.Sequential을 통해 간단하게 구현할 수 있다.


```python
seq_model = nn.Sequential(
    nn.Linear(1, 13),
    nn.Tanh(),
    nn.Linear(13, 1)
)
seq_model
```




    Sequential(
      (0): Linear(in_features=1, out_features=13, bias=True)
      (1): Tanh()
      (2): Linear(in_features=13, out_features=1, bias=True)
    )



13이 뭐냐고? 입력과 출력의 피처 수는 여전히 1이지만, 중간에 있는 은닉층은 그렇지 않다. 위의 모델은 피처 수가 13인 은닉층을 사용하는 것이다. 어떤 수를 사용해도 상관없지만, 앞의 모듈 뒤 파라미터와 뒤의 모듈 앞 파라미터가 같아야 한다는 것은 자명하다.

이제 모델의 파라미터를 살펴보자. 전부 살펴보기는 어려우니 param.shape만 보자.


```python
[param.shape for param in seq_model.parameters()]
```




    [torch.Size([13, 1]), torch.Size([13]), torch.Size([1, 13]), torch.Size([1])]



여러 개의 서브 모듈로 구성된 모델의 파라미터를 추적할 때는, 파라미터를 이름으로 식별가능하게 하면 편리하다. named_paraemters 메소드가 이를 위해 존재한다.


```python
for name, param in seq_model.named_parameters():
    print(name, param.shape)
```

    0.weight torch.Size([13, 1])
    0.bias torch.Size([13])
    2.weight torch.Size([1, 13])
    2.bias torch.Size([1])
    

여기서 숫자는 Sequential에 나타난 모듈의 순서를 따른다. 또는 아예 nn.Sequential 인자에 OrderedDict를 넣어서 거기서 모듈의 이름을 정해도 된다. 이렇게 말이다.


```python
from collections import OrderedDict
seq_model = nn.Sequential(OrderedDict([
    ('hidden_linear', nn.Linear(1, 13)),
    ('hidden_activ', nn.Tanh()),
    ('output_linear', nn.Linear(13, 1))
]))

seq_model
```




    Sequential(
      (hidden_linear): Linear(in_features=1, out_features=13, bias=True)
      (hidden_activ): Tanh()
      (output_linear): Linear(in_features=13, out_features=1, bias=True)
    )



그러면


```python
for name, param in seq_model.named_parameters():
    print(name, param.shape)
```

    hidden_linear.weight torch.Size([13, 1])
    hidden_linear.bias torch.Size([13])
    output_linear.weight torch.Size([1, 13])
    output_linear.bias torch.Size([1])
    

이렇게 이름을 지정하면 서브 모듈을 속성처럼 사용해 특정 파라미터에 접근할 수 있다.


```python
seq_model.output_linear.bias
```




    Parameter containing:
    tensor([-0.2397], requires_grad=True)



파라미터를 추적하거나 grad를 파악하기에 유용한 방법이다.

예를 들어, 앞선 코드들처럼 훈련시키고 나서 은닉층 선형 영역의 grad를 확인하고 싶으면...


```python
optimizer = optim.SGD(seq_model.parameters(), lr=1e-3)
training_loop(
    n_epochs=5000,
    optimizer=optimizer,
    model=seq_model,
    loss_fn=nn.MSELoss(),
    t_u_train=t_un_train,
    t_c_train=t_c_train,
    t_u_val=t_un_val,
    t_c_val=t_c_val
)
print('output:', seq_model(t_un_val))
print('hidden weight grad:', seq_model.hidden_linear.weight.grad)
```

    Epoch 1, Training loss 204.7793,Validation loss 254.9063
    Epoch 1000, Training loss 3.5819,Validation loss 7.2766
    Epoch 2000, Training loss 3.3309,Validation loss 9.9590
    Epoch 3000, Training loss 2.0153,Validation loss 6.9415
    Epoch 4000, Training loss 1.6151,Validation loss 5.6787
    Epoch 5000, Training loss 1.5373,Validation loss 5.1495
    output: tensor([[-0.1706],
            [20.5834]], grad_fn=<AddmmBackward0>)
    hidden weight grad: tensor([[-0.1942],
            [-0.0022],
            [ 0.2528],
            [-0.0039],
            [ 0.0112],
            [-0.2452],
            [-0.2360],
            [-0.2408],
            [-0.2469],
            [ 0.0015],
            [ 0.1999],
            [ 0.0023],
            [ 0.0091]])
    

이제 이 모델이 선형 모델과 비교해서 어떤지 평가해보자!


```python
from matplotlib import pyplot as plt

t_range = torch.arange(20., 90.).unsqueeze(1)
# 20부터 89까지의 자연수가 담긴 텐서에 여분의 1번째 차원 추가

fig = plt.figure(dpi=600)
plt.xlabel("Fahrenheit")
plt.ylabel("Celsius")
plt.plot(t_u.numpy(), t_c.numpy(), 'o')
plt.plot(t_range.numpy(), seq_model(0.1*t_range).detach().numpy(), 'c-')
plt.plot(t_u.numpy(), seq_model(0.1 * t_u).detach().numpy(), 'kx')
plt.show()
```


    
![png](../assets/images/6-Neural%20Network_files/6-Neural%20Network_45_0.png)
    


그림을 보면 약간 과적합 느낌이 난다. 측정값이 얼마 되지 않아 상대적으로 적합에 필요한 양보다 파라미터가 많기 때문일 수 있다. 그래도 나쁘진 않다!

## 결론

이번 에피소드에서는 torch.nn으로 선형 모델을 구현하고 훈련을 진행해 보고, 신경망도 훈련시켜 보았다. 다음 에피소드에서는 이미지 학습에 대해 다뤄보자.

[이전 에피소드로](/python/pytorch1) [다음 에피소드로](/python/pytorch3)
